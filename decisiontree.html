<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Decision Trees</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" />Fradulent Credit Card Transactions</span><span class="title"></span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="introduction.html">Introduction</a></li>
							<li><a href="data_preparation.html">Data Preparation</a></li>
							<li><a href="EDA.html">Exploratory Data Analysis</a></li>
							<li><a href="model_exp.html">Model Explanation</a></li>
							<li><a href="conclusion.html">Learning and Outcomes</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Decision Trees</h1>
							<span class="image main"><img src="images/pic13.jpg" alt="" /></span>
							<h2>Overview</h2>
							<p align="justify">
								Decision trees are a type of machine learning algorithm used for both classification and regression problems. 
                                They work by recursively splitting the data based on the most informative attributes or features, in order 
                                to create a tree-like model of decisions and their consequences.
								<br>
								The basic idea behind a decision tree is to build a tree-like model of decisions and their possible consequences, 
                                with each internal node representing a test on an attribute, each branch representing the outcome of the test, and each leaf 
                                node representing a class label or a numerical value.<br>
                                To train a decision tree, the entire dataset is treated as the root node, and then it recursively splits the data
                                based on the most informative attribute, i.e., the attribute that best separates the classes or reduces the variance in 
                                the target variable which are based on either information gain, entropy, gini, etc.
								<br>
                                Predictions with a decision tree, the algorithm traverses the tree from the root node to a leaf node, based on the 
                                attribute tests and outcomes, until it reaches a final prediction. The prediction can be a class label in case of classification,
                                 or a numerical value in case of regression.
								<br>
								<br>
                                The commonly used metrics in decision trees to determine the best split of data at each internal node
                                <ul>
                                    <li>Gini Index - It measures the probability of misclassifying a randomly chosen data point. It ranges from 0 to 1, 
                                        where 0 means all the data points belong to the same class, and 1 means the data points are uniformly distributed across all 
                                        classes</li>
                                    <li>Entropy - It is measure of purity that is commonly used in decision trees. Entropy measures the degree of disorder
                                         or uncertainty in a set of data. It ranges from 0 to 1, where 0 means all the data points belong to the same class, and 1 means
                                          the data points are equally distributed across all classes</li>  
                                    <li>Information Gain - It is a measure of how much the entropy or impurity of the data decreases after a split.
                                         It is used to determine which attribute or feature to choose as the splitting criterion. The attribute or feature with the highest 
                                         information gain is chosen as the splitting criterion, as it provides the most informational power to separate the classes or reduce 
                                         the variance in the target variable</li>        
                                </ul>
                                <br>
                                <a href="https://thatascience.com/learn-machine-learning/gini-entropy/">
                                    <img src="images/dt_metrics.png" width="650" height="300" style="margin:30px 10px">
                                    </a>
                                    <br>
                                    <a href="https://www.quora.com/What-is-the-difference-between-information-gain-and-gini-index-When-should-I-apply-each-of-the-method">
                                        <img src="images/information_gain.webp" width="650" height="300" style="margin:30px 10px">
                                        </a>
							</p>
								<br>
								<br>

							<h2>Code Explanation</h2>
							<p align="justfiy">
								The basic idea is to partition the dataset into two subsets: one for training the model and another for testing its accuracy,
								 which is done using one of the libraries provided  <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">"from sklearn.model_selection import train_test_split"</a>. 
								 The training set 
								 is used to fit the model parameters, while the test set is used to evaluate the performance of the trained model on new, 
								 unseen data. In our case due to the imbalanced dataset(where it had just 492 fraudulent cases) we will use sampling method in this case
								 downsampling to make the majority and minority class label have same proportion. Here the split is done using a paramater <b>stratify</b>
								 which basically returns train and test with same proportions of class labels 
								 as the input dataset. Moreover the train and test is split in 70%-30% as describe in the parameter <b>test_size</b>
								 Where <b>random_state</b> defines the random sampling of the train and test datasets and produces same results every time it is run.
								 <br>
								 <img src="images/train_test.jpeg" width="950" height="100" style="margin:30px 10px">
								
							</p>
							<br>
							<h2>Cleaned Data</h2>
							<p align="justify">
								<img src="images/naivebayesdata.jpeg" width="800" height="300" style="margin:30px 10px">
							</p>
							<br>
							<h2>Training & Testing Data</h2>
							<p align="justify">
								<img src="images/naivebayes_features.jpeg" width="700" height="400" style="margin:30px 10px">
								<br>
								<img src="images/naivebayes_label.jpeg" width="500" height="300" style="margin:30px 10px">
							</p>
							<br>

							<p align="justify">
								Our problem statement is to train our model to identify the fraudulent transactions on the unseen data, a classification 
								algorithm will identify and classify the transactions 
								as fraudulent or non-fraudulent. Let's see this using a Decision Tree Classifier.

								After fitting the Decision Tree model against our training data shown above, the next step is to make prediction on unseen data 
								which is testing data.

								<h3>Confusion Matrix</h3>
								<img src="images/dt_confusion.png" width="650" height="400" style="margin:30px 10px">
								<br>
								From the confusion matrix above, we get the idea of our True Positive Rate and True Negative Rate, Accuracy, Senstivity, Specificity.
								To check if our model is performing well, there is cost associated to whether having a false negative case or false positive case is costly for us.
								Which means predicting a fraud case as non-fraudulent (i.e false negative) or non-fraud as fraud (i.e false positive) is beneficial for us,
								In this case we will want to decrease the false negative cases which in turn is costly for us when detecting on unseen data, as 
								we will incur a financial loss due to the cases which were not detected as fraudulent. From the above confusion matrix, there are less
                                false negatives cases than it was in our Naive Bayes Classifier, thus decision tree did a great job in identifying more true fraudulent cases.

								The most common metrics to use for imbalanced dataset are:
								<ul>
								<li>F1 score</li>
								<li>Precision</li>
								<li>Recall</li>
								<li>AUC score (AUC ROC)</li>
								<li>Average precision score (AP)

								</li></ul>
								<br>	
								<br>
								Let's have a look at our performance metrics.<br>
								<img src="images/dt_accuracy.jpeg" width="650" height="400" style="margin:30px 10px">
								<br>
								<img src="images/dt_roc.png" width="650" height="400" style="margin:30px 10px">	
							</p>
                            <h2>Decision Tree Visualisation</h2>
                            <p align="justify">
                                Decision Tree on full dataset
                                <br>
                                <embed src="images/decision_tree.pdf" type="application/pdf" width="100%" height="700px" 
                                style="border:none;" frameborder="0" scrolling="no"
                                 title="Decision Tree PDF Document">
<br><br>
                                 Decision Tree on downsampled dataset
                                 <br>
                                 <embed src="images/decision_tree2.pdf" type="application/pdf" width="100%" height="700px" 
                                 style="border:none;" frameborder="0" scrolling="no"
                                  title="Decision Tree PDF Document"> <br><br>

                                  Decision Tree on most important features
                                 <br>
                                 <embed src="images/decision_tree3.pdf" type="application/pdf" width="100%" height="700px" 
                                 style="border:none;" frameborder="0" scrolling="no"
                                  title="Decision Tree PDF Document">


                            </p>
                            <h3>Code</h3>
                            <p align="justify">
                               You can find code for the Decision Tree Classifier covered in this space here:
                                <ul>  
                                    
                           <li><a href="https://github.com/anshsach2208/fraudulent_transactions/blob/main/classification.ipynb">
                                       Classification - Naive Bayes and Decision Trees</a></li>
                            <li><a href="https://github.com/anshsach2208/fraudulent_transactions/blob/main/decision_trees.ipynb">
                                    Decision Trees Visualisation</a></li> 
                                    <li><a href="https://github.com/anshsach2208/fraudulent_transactions/blob/main/datasets/creditcard-modified.csv">
                                        Dataset
                                    </a>          

							<h2>Conclusion</h2>
							<p align="justify">
								From the performance metrics we can conclude that Decision Trees  did a great job than Naive Bayes in indentifying most of the 
								fraudulent transactions 92 out of 110, although we can also test against upsampled dataset or by doing feature importance to increase our recall score 
								such that model is more accurate in predicting fraudlent transactions. Our Decision Trees model gave similar accuracy of 89%.
								Though decision tree had a better recall score for fraudulent transactions. In case of fraudulent transactions recall is better favoured than precision because there is no harm in alerting a non-fraud
								transaction as fraud, but there will be more loss if a fraud transactions is labelled as non-fraud.
							</p>
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="field half">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="field">
											<textarea name="message" id="message" placeholder="Message"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="primary" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<!-- <li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li> -->
									<!-- <li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li> -->
									<li><a href="https://www.instagram.com/ansh_sach2208/" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li> -->
									<li><a href="https://github.com/anshsach2208" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li> -->
									<!-- <li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li> -->
									<li><a href="mailto:anshsachdeva748@gmail.com" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
										<li><a href="https://www.linkedin.com/in/anshsachdeva98/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>CSCI 5622-073 - Machine Learning</li><li>Fraudulent Credit Card Transaction By Ansh Sachdeva</li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>