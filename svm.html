<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SVM</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" />Fradulent Credit Card Transactions</span><span class="title"></span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="introduction.html">Introduction</a></li>
							<li><a href="data_preparation.html">Data Preparation</a></li>
							<li><a href="EDA.html">Exploratory Data Analysis</a></li>
							<li><a href="model_exp.html">Model Explanation</a></li>
							<li><a href="conclusion.html">Learning and Outcomes</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Support Vector Machines</h1>
							<span class="image main"><img src="images/pic13.jpg" alt="" /></span>
							<h2>Overview</h2>
							<p align="justify">
								
                                <h3> Why are SVM linear Seperators?</h3>
                                Support vector machines (SVMs) are often used as linear classifiers as their objective is to find a linear decision boundary that separates the classes with the largest margin. 
                                This is known as the maximum margin hyperplane (MMH) and it is a linear separator.The MMH is the line, plane or hyperplane that maximizes the distance between the two classes of 
                                data points in a high-dimensional space. In two dimensions, the MMH is simply a line that separates the two classes of points, while in higher dimensions, 
                                it is a hyperplane that separates the two classes.
                                If the data is not linearly separable, SVMs uses a technique called kernel methods to transform the data into a higher-dimensional space where it becomes 
                                linearly separable.<br>
                                <br>
                                <h3> How does kernel work in SVM?</h3>
                                In order to move the input data into a higher-dimensional space, where linear separability is more possible, the kernel function is applied. The decision boundary that divides the classes with the 
                                largest margins is known as the maximum margin hyperplane (MMH), and it is discovered using the converted data.
                                <br>
                                The inner product between two data points in the transformed feature space is computed by the kernel function, which is comparable to calculating how similar the two data points are. The linear kernel, 
                                polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel are the most often utilized kernel functions.
                                <br><br>
                                <h3>Why is calculating dot product is so essential while doing SVM?</h3>
                                The ability to compute the similarity between two data points in a high-dimensional space without explicitly computing the coordinates of the data points in that space makes the dot 
                                product essential to the use of the kernel in SVMs.
                                <br>
                                The kernel function makes it possible for us to calculate the dot product between the data points in the feature space rather than the coordinates of the data points in the feature space. 
                                The similarity between the two data points in the feature space is determined by the dot product. The more similar the two data points are in the feature space, the greater the dot product.
                                Without directly calculating the coordinates of the data points in the feature space, the kernel function enables us to use the SVM algorithm. As a result, even when the explicit computation 
                                of the coordinates in that space would be impractical, SVMs can be used with very high-dimensional feature spaces.

                                <br><br>
                                <a href="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FAn-illustration-of-support-vector-machine-SVM-intuition_fig1_343397484&psig=AOvVaw0E-A6Z0gFMS9yPO7AI09fI&ust=1681866074771000&source=images&cd=vfe&ved=0CBIQjhxqFwoTCPD1jsOdsv4CFQAAAAAdAAAAABAD">
                                    <img src="images/svm1.png" width="550" height="300" style="margin:30px 10px">
                                    </a>
                                    
                                    <a href="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FSupport-vector-machine-SVM-3_fig2_334365773&psig=AOvVaw0E-A6Z0gFMS9yPO7AI09fI&ust=1681866074771000&source=images&cd=vfe&ved=0CBIQjhxqFwoTCPD1jsOdsv4CFQAAAAAdAAAAABAI">
                                        <img src="images/svm2.png" width="550" height="300" style="margin:30px 10px">
                                        </a>
                                        <br>
                                        <br>
                               <h3>Polynomial Kernek for r=1 and d=2, a and b are 2D </h3>
                                   <br>
                                a is (a1, a2) and b is (b1, b2) [polynomial kernel= (aTb + r)^d] <br>
                                Let r = 1 and let d = 2 <br>
                                (aTb + 1)2 = (aTb +1)(aTb +1) = <br>
                                (aTb)(aTb) + 1/2aTb + 1/2aTb + 1 = <br>
                                (aTb)(aTb) + aTb + 1 =<br>
                                (a1b1 + a2b2)(a1b1+a2b2) + (a1b1 + a2b2) + 1 =<br>
                                a1^2b1^2 + 2a1a2b1b2 + a2^2b2^2 + a1b1 + a2b2 + 1<br>
                                Now  let's see how to write dot product for it<br>
                                [a1^2 , sqrt(2)a1a2, a2^2, a1, a2, 1] dot [b1^2 , sqrt(2)b1b2 , b2^2, b1, b2, 1]<br>
                                <br>
                                Now – let’s check the dot product we get directly from the kernel vs the dot product we
                                get after the transformation.<br>
                                <br>
                                Let a = (3,2) and let b = (2,3)
                                where r = 1 and d = 2 so that K(a,b) = (aTb + 1)^2
                                If we plug in a and b directly we should get the dot product from the
                            transformed space!
                                <br>
                                    ((3, 2) dot (2, 3) + 1)^2 = (12 + 1)^2 =(13)^2 = 169
                            OK - what happens if we transform a and b first and then get the dot
                            product in the transformed space. It should be the same that is it should be
                            169. <br>

                            Let r = 1 and let d = 2 <br>
                            (ab + 1)^2 = a1^2b1^2 + 2a1a2b1b2 + a2^2b2^2 + a1b1 + a2b2 + 1 <br>
                            and the dot product is: <br>
                            [a1^2 , sqrt(2)a1a2, a2^2, a1, a2, 1] dot [b12 , sqrt(2)b1b2 , b2^2, b1, b2, 1] <br>
                            Let a = (3,2) and let b = (2,3) <br>
                            The transformation of a is <br>
                            [a1^2 , sqrt(2)a1a2, a2^2, a1, a2, 1] --> [9, sqrt(2)(3)(2), 4, 3, 2, 1] <br>
                            The transformation of b is <br>
                            [b1^2 , sqrt(2)b1b2 , b2^2, b1, b2, 1/2] --> [4, sqrt(2)(2)(3), 9, 2, 3, 1] <br>
                            Now – get the dot product<br>
                            [9, sqrt(2)(3)(2), 4, 3, 2, 1] dot [4, sqrt(2)(2)(3), 9, 2, 3, 1]
                            = 169
<br><br>
                               </h3>
                                <h3>Radial Basis Function</h3>
                                Due to its resemblance to the Gaussian distribution, RBF kernels are among the most extensively used kernels and the most versatile kind of kernelization. For two points X1 and X2, the RBF kernel function calculates 
                                their similarity or how near they are to one another.
                                where,<br>
                                1. ‘σ’ is the variance and our hyperparameter<br>
                                2. ||X₁ - X₂|| is the Euclidean (L₂-norm) Distance between two points X₁ and X₂
                                <br>
                                <br>
                                <h3>Polynomial Function</h3>
                                An SVM kernel known as a polynomial kernel maps the data into a higher-dimensional space using a polynomial function. This is done by taking the dot product of the polynomial function 
                                in the new space and the original space's data points.
                                The polynomial kernel is often used in SVM classification problems where the data is not linearly separable. By mapping the data into a higher-dimensional space, the polynomial kernel
                                 can sometimes find a hyperplane that separates the classes.

                                The polynomial kernel has a number of parameters that can be tuned to improve its performance, including the degree of the polynomial and the coefficient of the Polynomial
                                <br>
                                For degree d polynomials, the polynomial kernel is defined as:
                                where c is a constant and X1 and X2 are vectors in the original space.
                                <br>
                                <br>
                                <h3> Polynomial and Radial Basis Function Equations</h3>  
                               <a href="https://www.google.com/url?sa=i&url=https%3A%2F%2Fjournals.plos.org%2Fplosone%2Farticle%2Ffigures%3Fid%3D10.1371%2Fjournal.pone.0235885&psig=AOvVaw3hMsLaOmD9BLsXm935hSrM&ust=1681869826566000&source=images&cd=vfe&ved=0CBIQjhxqFwoTCOD9o8Crsv4CFQAAAAAdAAAAABAD">
                                <center><img src="images/kernel.png" width="700" height="300" style="margin:30px 10px"></center>
                                </a>  
                                <br>
                                <br>
							</p>
								<br>
								<br>

							<h2>Code Explanation</h2>
							<p align="justfiy">
								The basic idea is to partition the dataset into two subsets: one for training the model and another for testing its accuracy,
								 which is done using one of the libraries provided  <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">"from sklearn.model_selection import train_test_split"</a>. 
								 The training set 
								 is used to fit the model parameters, while the test set is used to evaluate the performance of the trained model on new, 
								 unseen data. In our case due to the imbalanced dataset(where it had just 492 fraudulent cases) we will use sampling method in this case
								 downsampling to make the majority and minority class label have same proportion. Here the split is done using a paramater <b>stratify</b>
								 which basically returns train and test with same proportions of class labels 
								 as the input dataset. Moreover the train and test is split in 70%-30% as describe in the parameter <b>test_size</b>
								 Where <b>random_state</b> defines the random sampling of the train and test datasets and produces same results every time it is run.
								 <br>
								 <img src="images/train_test.jpeg" width="950" height="100" style="margin:30px 10px">
								
							</p>
							<br>
							<h2>Cleaned Data</h2>
							<p align="justify">
								<img src="images/naivebayesdata.jpeg" width="800" height="300" style="margin:30px 10px">
							</p>
							<br>
							<h2>Training & Testing Data</h2>
							<p align="justify">
								<img src="images/naivebayes_features.jpeg" width="700" height="400" style="margin:30px 10px">
								<br>
								<img src="images/naivebayes_label.jpeg" width="500" height="300" style="margin:30px 10px">
							</p>
							<br>

							<p align="justify">
								Our problem statement is to train our model to identify the fraudulent transactions on the unseen data, a classification 
								algorithm will identify and classify the transactions 
								as fraudulent or non-fraudulent. Let's see this using a Support Vector Machine Classifier.

								After fitting the Support Vector Machine model against our training data shown above, the next step is to make prediction on unseen data 
								which is testing data.

								<h3>Confusion Matrix</h3>
								<img src="images/svm_cm.png" width="650" height="400" style="margin:30px 10px">
								<br>
								From the confusion matrix above, we get the idea of our True Positive Rate and True Negative Rate, Accuracy, Senstivity, Specificity.
								To check if our model is performing well, there is cost associated to whether having a false negative case or false positive case is costly for us.
								Which means predicting a fraud case as non-fraudulent (i.e false negative) or non-fraud as fraud (i.e false positive) is beneficial for us,
								In this case we will want to decrease the false negative cases which in turn is costly for us when detecting on unseen data, as 
								we will incur a financial loss due to the cases which were not detected as fraudulent.

								The most common metrics to use for imbalanced dataset are:
								<ul>
								<li>F1 score</li>
								<li>Precision</li>
								<li>Recall</li>
								<li>AUC score (AUC ROC)</li>
								<li>Average precision score (AP)

								</li></ul>
								<br>	
								<br>
								Let's have a look at our performance metrics.<br>
								<img src="images/svm_perf.png" width="650" height="400" style="margin:30px 10px">
								<br>
								
                                <h3>Confusion Matrix for different kernels and c values</h3>
                                <br>
                                
                                Confusion Matrix for C=0.1 and kernel=rbf
                                <br>
								<img src="images/svm_accuracy.png" width="650" height="400" style="margin:30px 10px">
                                <br>
                                <br>
                                Confusion Matrix for C=10 and kernel=linear
                                <br>
                                <img src="images/svm_fraud.png" width="650" height="400" style="margin:30px 10px">
                                <br>
                                <br>
                                From the above SVM models for different kernels and different c value we can say that for value c=10 and kernel=linear 
                                we have the best fraud accuracy which is 91.2% and overall accuracy of 96.6%

                                Though the best overall accuracy was achieved by the kernel=rbf with c value=0.1 but our goal is to have maximum accuracy against fraudulent
                                transactions where it performed poorly with 78% accuracy. So we would probably chose the linear kernel model with c=10

							</p>
                     
                            <h3>Code</h3>
                            <p align="justify">
                                <ul>  
                                    
                           <li><a href="https://github.com/anshsach2208/fraudulent_transactions/blob/main/classification.ipynb">
                                       Classification - Support Vector Machine</a></li>
                                    <li><a href="https://github.com/anshsach2208/fraudulent_transactions/blob/main/datasets/creditcard-modified.csv">
                                        Dataset
                                    </a></li>
                                </ul>

                            </p>           

							<h2>Conclusion</h2>
							<p align="justify">
								From the performance metrics we can conclude that SVM did a great job in indentifying most of the 
								fraudulent transactions 135 out of 148. Moreover it has a better recall, in case of fraudulent transactions recall is better favoured than precision because there is no harm in alerting a non-fraud transaction as fraud, 
                                but there will be more loss if a fraud transactions is labelled as non-fraud
							</p>
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="field half">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="field">
											<textarea name="message" id="message" placeholder="Message"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="primary" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<!-- <li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li> -->
									<!-- <li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li> -->
									<li><a href="https://www.instagram.com/ansh_sach2208/" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li> -->
									<li><a href="https://github.com/anshsach2208" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li> -->
									<!-- <li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li> -->
									<li><a href="mailto:anshsachdeva748@gmail.com" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
										<li><a href="https://www.linkedin.com/in/anshsachdeva98/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>CSCI 5622-073 - Machine Learning</li><li>Fraudulent Credit Card Transaction By Ansh Sachdeva</li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>