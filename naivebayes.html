<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Introduction</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" />Fradulent Credit Card Transactions</span><span class="title"></span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="introduction.html">Introduction</a></li>
							<li><a href="data_preparation.html">Data Preparation</a></li>
							<li><a href="EDA.html">Exploratory Data Analysis</a></li>
							<li><a href="model_exp.html">Model Explanation</a></li>
							<li><a href="conclusion.html">Learning and Outcomes</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Naive Bayes</h1>
							<span class="image main"><img src="images/pic13.jpg" alt="" /></span>
							<h2>Multinomial Naive Bayes</h2>
							<p align="justify">
								Multinomial Naive Bayes (MNB) is a probabilistic algorithm used for text classification. 
								It is one of the simplest and most effective algorithms for this task. The algorithm is based on Bayes' theorem, 
								which describes the probability of an event, based on prior knowledge of conditions that might be related to the event.
								<br>
								The MNB algorithm is trained using a set of labeled documents. Each document in the set is represented by a vector of word counts, where the count for
 								each word represents the number of times that word appears in the document. The training process involves estimating the probabilities of each word 
								occurring in each class (i.e., category or label). This is done by counting the number of times each word appears in each class and normalizing by the 
								total number of words in that class. This gives us the probability of each word occurring in each class, which is used to calculate the probability of a 
								document belonging to a particular class.<br>
								To make a prediction for a new document, the MNB algorithm calculates the probability of the document belonging to each class based on the word probabilities 
								estimated during training. It then chooses the class with the highest probability as the predicted class for the document.
								One of the key assumptions of the MNB algorithm is the independence of the features (i.e., words) given the class. This means that the occurrence of one word
								in a document does not affect the probability of another word occurring in that same document.
								<br>
								<br>
								<a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">
								<img src="images/naivebayes.webp" width="650" height="300" style="margin:30px 10px">
								</a>
								<a href="https://github.com/JonathanRadotski/multinomial_naivebayes">
									<img src="images/naivebayes2.png" width="450" height="300" style="margin:30px 10px">
								</a>
								<br>
								<br>
							</p>
								<h2>Smoothing in Naive Bayes</h2>
								<p align="justify">
									With a set of input features, Naive Bayes determines the likelihood of a class given in the input. 
									In certain circumstances, a feature may not be present in the training data for a specific class, leading 
									to a probability of zero. This can be problematic when attempting to make predictions because the sum of the 
									probabilities for all features will equal zero, giving that class an overall probability of zero.
									<br>
									Smoothing is used to address this problem by adjusting the probability estimates for features that have 
									zero counts in the training data. The idea is to add a small constant value (often called the smoothing 
									parameter or Laplace smoothing parameter) to the count of each feature, which has the effect of shifting 
									the probability estimates away from zero. This helps to avoid overfitting and makes the model more robust 
									to unseen data.
									<br>
									<br>
									<iframe width="560" height="315" src="https://www.youtube.com/embed/jSaU_iDB1Ds" 
											frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; 
											gyroscope; picture-in-picture" allowfullscreen>
									</iframe>
								</p>
								<br>
								<br>
								<h2>Bernoulli Naive Bayes</h2>
								<p align="justify">
									The primary characteristic of Bernoulli Naive Bayes is that only binary values—such as true or false, yes or no, 
									success or failure, 0 or 1—are accepted for features. So, we prefer to use the Bernoulli Naive Bayes classifier 
									when the feature values are binary.
									In Bernoulli Naive Bayes, each input feature is assumed to have a binary probability distribution, where the probability 
									of the feature being present is represented by a parameter called the feature probability. The model calculates the 
									probability of each class given the input features using Bayes' theorem, which states that the probability of a 
									a class label given the input features is proportional to the 
									probability of the evidence given the class label, multiplied by the prior probability of  class labels.
								</p>
							   <br><br><a href="https://heena-sharma.medium.com/naive-bayes-in-machine-learning-684e38a96e8d">
								<img src="images/naivebayes3.jpeg" width="450" height="200" style="margin:30px 10px">
							</a>
							<h2>Code Explanation</h2>
							<p align="justfiy">
								The basic idea is to partition the dataset into two subsets: one for training the model and another for testing its accuracy,
								 which is done using one of the libraries provided  <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">"from sklearn.model_selection import train_test_split"</a>. 
								 The training set 
								 is used to fit the model parameters, while the test set is used to evaluate the performance of the trained model on new, 
								 unseen data. In our case due to the imbalanced dataset(where it had just 492 fraudulent cases) we will use sampling method in this case
								 downsampling to make the majority and minority class label have same proportion. Here the split is done using a paramater <b>stratify</b>
								 which basically returns train and test with same proportions of class labels 
								 as the input dataset. Moreover the train and test is split in 70%-30% as describe in the parameter <b>test_size</b>
								 Where <b>random_state</b> defines the random sampling of the train and test datasets and produces same results every time it is run.
								 <br>
								 <img src="images/train_test.jpeg" width="950" height="100" style="margin:30px 10px">
								
							</p>
							<br>
							<h2>Cleaned Data</h2>
							<p align="justify">
								<img src="images/naivebayesdata.jpeg" width="800" height="300" style="margin:30px 10px">
							</p>
							<br>
							<h2>Training & Testing Data</h2>
							<p align="justify">
								<img src="images/naivebayes_features.jpeg" width="700" height="400" style="margin:30px 10px">
								<br>
								<img src="images/naivebayes_label.jpeg" width="500" height="300" style="margin:30px 10px">
							</p>
							<br>

							<p align="justify">
								Our problem statement is to train our model to identify the fraudulent transactions on the unseen data, a classification 
								algorithm will identify and classify the transactions 
								as fraudulent or non-fraudulent. Let's see this using a Naive Bayes Classifier.

								After fitting the Naive Bayes model against our training data shown above, the ext step is to make prediction on unseen data 
								which is testing data.

								<h3>Confusion Matrix</h3>
								<img src="images/nb_confusion.png" width="650" height="400" style="margin:30px 10px">
								<br>
								From the confusion matrix above, we get the idea of our True Positive Rate and True Negative Rate, Accuracy, Senstivity, Specificity.
								To check if our model is performing well, there is cost associated to whether having a false negative case or false positive case is costly for us.
								Which means predicting a fraud case as non-fraudulent (i.e false negative) or non-fraud as fraud (i.e false positive) is beneficial for us,
								In this case we will want to decrease the false negative cases which in turn is costly for us when detecting on unseen data, as 
								we will incur a financial loss due to the cases which were not detected as fraudulent.

								The most common metrics to use for imbalanced dataset are:
								<ul>
								<li>F1 score</li>
								<li>Precision</li>
								<li>Recall</li>
								<li>AUC score (AUC ROC)</li>
								<li>Average precision score (AP)

								</li></ul>
								<br>	
								<br>
								Let's have a look at our performance metrics.<br>
								<img src="images/nb_accuracy.jpeg" width="650" height="400" style="margin:30px 10px">
								<br>
								<img src="images/nb_roc.png" width="650" height="400" style="margin:30px 10px">	
							</p>

							<h2>Conclusion</h2>
							<p align="justify">
								From the performance metrics we can conclude that naive bayes did a great job in indentifying most of the 
								fraudulent transactions 90 out of 110, although we can also test upsampling or feature importance to increase our recall score 
								such that model is more accurate in predicting fraudlent transactions. Our Naive Bayes model gave an accuracy of 89%.
								In case of fraudulent transactions recall is better favoured than precision because there is no harm in alerting a non-fraud
								transaction as fraud, but there will be more loss if a fraud transactions is labelled as non-fraud.
							</p>
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="field half">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="field">
											<textarea name="message" id="message" placeholder="Message"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="primary" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<!-- <li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li> -->
									<!-- <li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li> -->
									<li><a href="https://www.instagram.com/ansh_sach2208/" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li> -->
									<li><a href="https://github.com/anshsach2208" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
									<!-- <li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li> -->
									<!-- <li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li> -->
									<li><a href="mailto:anshsachdeva748@gmail.com" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
										<li><a href="https://www.linkedin.com/in/anshsachdeva98/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>CSCI 5622-073 - Machine Learning</li><li>Fraudulent Credit Card Transaction By Ansh Sachdeva</li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>